---
# file: roles/kubernetes-master/tasks

- name: iniciar el control-plane de kubernetes y la red para los pods
  command: kubeadm init --apiserver-advertise-address={{ apiserver_address }} --pod-network-cidr "{{ pod_network_cidr }}"
  become: yes  

- name: registrando usuario
  command: whoami
  register: user
  delegate_to: 127.0.0.1
  
- name: cluster config directorio
  file:
    path: $HOME/.kube
    state: directory
    owner: "{{ user.stdout }}"
    group: "{{ user.stdout }}"
    mode: '770'

# no se puede usar el modulo copy ya que el direcorio /etc/kubernetes/admin.conf
# tiene los permisos 600 y no pude ser leido por un usuario no root      
- name: cluster config copiar el directorio de configuracion
  command: cp /etc/kubernetes/admin.conf /home/"{{ user.stdout }}"/.kube/config 
  become: yes
  args:
    warn: false

- name: configuracion de permisos sobre /home/{{ user.stdout }}/.kube/config
  file:
    path: /home/{{ user.stdout }}/.kube/config
    owner: "{{ user.stdout }}"
    group: "{{ user.stdout }}"
    mode: '660'
  become: yes

- name: instalando SDN (calico)
  command: kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml

- name: obteniendo  los custom resources para calico
  get_url:
    url: https://docs.projectcalico.org/manifests/custom-resources.yaml
    dest: $HOME/.kube/custom-resources.yaml

#reemplaza la red por defecto con la red de la variable  "pod_network_cidr"
- name: customizando custom-resources.yaml con red de los pods
  replace:
    path: $HOME/.kube/custom-resources.yaml
    regexp: '(?:[0-9]{1,3}\.){3}[0-9]{1,3}\/[1-9]{2}'
    replace: "{{ pod_network_cidr }}"
    backup: yes  
  
- name: instalando los custom resources para calico
  command: kubectl apply -f $HOME/.kube/custom-resources.yaml
  ignore_errors: yes

- name: desplegando el ingress controller (haproxy)
  command: kubectl apply -f https://raw.githubusercontent.com/haproxytech/kubernetes-ingress/v1.5/deploy/haproxy-ingress.yaml
  ignore_errors: yes 
  
- name: generando token para conectar los workers al cluster
  command: kubeadm token create --print-join-command
  register: kubernetes_worker_join_cmd
  become: yes

- name: Exportar el comando para unir los workers a una variable de tipo 'host-fact
  set_fact: kubernetes_worker_join_cmd={{ kubernetes_worker_join_cmd.stdout }}
  